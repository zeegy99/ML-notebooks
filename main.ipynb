{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09213e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('mnist_train.csv')\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17059f73",
   "metadata": {},
   "source": [
    "A simple Neural Network Trained to recognize MNIST dataset.\n",
    "\\\\\n",
    "\n",
    "1. Forward Propogation: This takes your inputs (X) and spits out some output ($\\hat{y}_i$). The way we will do this is we have 3 layers. We go from the input layer (784 neurons) $\\to$ Hidden Layer 1 (10 neurons) $\\to$ Output Layer (10 Neurons). \\\\\n",
    "\n",
    "\n",
    "    When going from input layer to Hidden layer 1, we will have some W (weights) with shape (10 x 784) multiplied with x (784 x n). What this means is that for each of the 784 features taken from X, there will be some combination of those weights that determine how many of our 10 neurons are activated. Then we use some non-linear activation function (ReLu in this case), otherwise our entire NN will be nested linear functions which is linear. \\\\\n",
    "    \n",
    "\n",
    "    Note: Consider f(g(x)). If f and g are both linear functions like mx + b, then f(g(x)) is also of the form mx + b which will not capture the complexities of our questions. \\\\\n",
    "\n",
    "    When going from Hidden Layer 1 to Output layer, we apply softmax which turns each output into a probability, with $\\sum neurons$ = 1. \\\\\n",
    "\n",
    "2. Back Prop: We are comparing the answer generated from forward prop to the real answer and go back and adjust the weights using gradient descent.\n",
    "\n",
    "    We want to turn our answer vector into similar form as our output, which means we will use one-hot. This turns y which has value 8 into a vector [0, 0, 0, 0, 0, 0, 0, 0, 1, 0].T \n",
    "\n",
    "    For our first loss function from softmax, we will use the cross-entropy-loss function defined as \\[ L = -\\sum (y_i log(\\hat{y}_i)) \\]\n",
    "\n",
    "\n",
    "    Then we apply gradient descent which moves these values from (learning rate) from where they are. Over ITERATIONS, these weights will become appropriately adjusted. \n",
    "\n",
    "\n",
    "3. Math:\n",
    "    1. For our softmax function (dW2), consider $\\frac{dL}{dW2} = \\frac{dL}{d\\hat{y}_i} \\cdot \\frac{d\\hat{y}_i}{dz_2} \\cdot \\frac{dz_2}{dW2}$. The reason why we need all of these partials is because we can rewrite L in terms of the next terms. \\\\\n",
    "\n",
    "        To solve this, lets look at \\[ L := -\\sum (y_i log(\\hat{y}_i)) \\] and break it down into its components. Here, $y_i$ is predetermined as 1 or 0, and we know that $\\hat{y}_i$ is our softmax function from above, $\\frac{e^{x_i}}{\\sum e^{x_j}}$. Thus, our first partial comes out to $\\frac{-y_i}{\\hat{y}_i}$. \\\\\n",
    "\n",
    "        For our next partial, (Softmax), we must consider this into 2 cases. On the top we have some $e^{x_i}$ term. So consider the case where our partial is with respect ti $x_i$ and then with respect to $x_j$ when $j \\neq i$. \\\\\n",
    "\n",
    "        In case 1: We will use simple calculus to solve this partial. Observe that \\[\n",
    "        \\hat{y}_i := \\frac{e^{x_i}}{\\sum e^{x_j}}\n",
    "        \\]. Thus, we can simplify our expression down to $\\hat{y}_i (1 - \\hat{y}_i)$. \\\\\n",
    "\n",
    "        Then, for the case when $i \\neq j$, we will get $-\\hat{y}_i \\hat{y}_j$. \\\\\n",
    "\n",
    "        Finally, for our last term $\\frac{dz_2}{dW2}$, remember that Z2 = W2 $\\cdot$ A1 (previous output layer) + $b_2$. So when taking the partial we are just left with $A1$. \\\\\n",
    "\n",
    "        Putting this all together, we get the desired result. \n",
    "\n",
    "    \\\\\n",
    "\n",
    "    Similarly for (b), just replace the last fraction with $dB2$. In fact, the partial comes out to be (1), which is even simpler! \\\\\n",
    "\n",
    "    For our next term (dz1), Apply the same process. I'll write out the fraction decomposition and you can work through the math again (or re-use from above). $\\frac{dL}{dZ1} = \\frac{dL}{dA1}\\frac{dA1}{dZ1}$. Then rewrite $\\frac{dL}{dA1} := \\frac{dL}{dZ2} \\frac{dZ2}{dA1}$. We have already computed these values. The only term that is somewhat confusing is $\\frac{dA1}{dz1}$. How do you take the derivative of a ReLu function? Well, since ReLu is 0 when x < 0, the derivative is 0 for x < 0. Similarly, since it is x when x $\\geq$ 0, the derivative is 1 for all x > 0. Thus, we can use a trick and write $Z > 0$ since True gets assigned to 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11e1779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [4 0 0 ... 0 0 0]]\n",
      "60000 785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array(dataset)\n",
    "m, n = data.shape\n",
    "\n",
    "print(data[:3])\n",
    "\n",
    "print(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc1d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c5881f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (784, 1000)\n",
      "(59000,) (784, 59000)\n"
     ]
    }
   ],
   "source": [
    "validate = data[:1000]\n",
    "train = data[1000:]\n",
    "\n",
    "\n",
    "validate = validate.transpose()\n",
    "y_validate = validate[0]\n",
    "x_validate = validate[1:]\n",
    "x_validate = x_validate / 255\n",
    "\n",
    "print(y_validate.shape, x_validate.shape)\n",
    "\n",
    "train = train.transpose()\n",
    "y_train = train[0]\n",
    "x_train = train[1:]\n",
    "x_train = x_train / 255\n",
    "\n",
    "print(y_train.shape, x_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff6154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propogation: \n",
    "\n",
    "#784 -(ReLu)-> 10 - Softmax -> 10 -> Output\n",
    "def initiate_param():\n",
    "    w1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    w2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    return w1, b1, w2, b2\n",
    "def Relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    val = (np.exp(z)) / (np.sum(np.exp(z), axis = 0))\n",
    "    return val\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = Relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "\n",
    "    return (z1, a1, z2, a2)\n",
    "\n",
    "\n",
    "\n",
    "#Backwards Prop\n",
    "def one_hot(Y):\n",
    "    one_hot_y = np.zeros((10, Y.size))\n",
    "    one_hot_y[Y, np.arange(Y.size)] = 1\n",
    "    \n",
    "    return one_hot_y\n",
    "\n",
    "def deriv_ReLu(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def backprop(Z1, A1, Z2, A2, W2, X, Y):\n",
    "    m = len(Y)\n",
    "    one_hot_y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_y\n",
    "    dW2 = 1 / m * np.dot(dZ2, A1.transpose())\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = np.dot(W2.transpose(), dZ2) * deriv_ReLu(Z1)\n",
    "    dW1 = 1 / m * np.dot(dZ1, X.transpose())\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "\n",
    "    return db1, dW1, db2, dW2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1 \n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    w1, b1, w2, b2 = initiate_param()\n",
    "    for i in range(iterations):\n",
    "        z1, a1, z2, a2 = forward_prop(w1, b1, w2, b2, X)\n",
    "        db1, dw1, db2, dw2 = backprop(z1, a1, z2, a2, w2, X, Y)\n",
    "\n",
    "        w1, b1, w2, b2 = update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha)\n",
    "        # print(\"This is new weights\", w1, b1, w2,)\n",
    "        if i % 100 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(a2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b921bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "0.10191525423728813\n",
      "Iteration:  100\n",
      "0.5928983050847457\n",
      "Iteration:  200\n",
      "0.7514237288135593\n",
      "Iteration:  300\n",
      "0.8073898305084746\n",
      "Iteration:  400\n",
      "0.834406779661017\n",
      "Iteration:  500\n",
      "0.8510677966101695\n",
      "Iteration:  600\n",
      "0.8618813559322034\n",
      "Iteration:  700\n",
      "0.8697966101694915\n",
      "Iteration:  800\n",
      "0.8760677966101695\n",
      "Iteration:  900\n",
      "0.8807966101694915\n"
     ]
    }
   ],
   "source": [
    "w1, b1, w2, b2 = gradient_descent(x_train, y_train, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d18c40c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894\n"
     ]
    }
   ],
   "source": [
    "#Now comprae against the training data\n",
    "_,_,_,a2 = forward_prop(w1, b1, w2, b2, x_validate)\n",
    "predictions = get_predictions(a2)\n",
    "acc = get_accuracy(predictions, y_validate)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54226c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
